num_healthy_workers,num_in_flight_async_sample_reqs,num_remote_worker_restarts,num_agent_steps_sampled,num_agent_steps_trained,num_env_steps_sampled,num_env_steps_trained,num_env_steps_sampled_this_iter,num_env_steps_trained_this_iter,num_env_steps_sampled_throughput_per_sec,num_env_steps_trained_throughput_per_sec,timesteps_total,num_env_steps_sampled_lifetime,num_agent_steps_sampled_lifetime,num_steps_trained_this_iter,agent_timesteps_total,done,training_iteration,trial_id,date,timestamp,time_this_iter_s,time_total_s,pid,hostname,node_ip,time_since_restore,iterations_since_restore,info/learner/Agent0/learner_stats/allreduce_latency,info/learner/Agent0/learner_stats/grad_gnorm,info/learner/Agent0/learner_stats/cur_kl_coeff,info/learner/Agent0/learner_stats/cur_lr,info/learner/Agent0/learner_stats/total_loss,info/learner/Agent0/learner_stats/policy_loss,info/learner/Agent0/learner_stats/vf_loss,info/learner/Agent0/learner_stats/vf_explained_var,info/learner/Agent0/learner_stats/kl,info/learner/Agent0/learner_stats/entropy,info/learner/Agent0/learner_stats/entropy_coeff,info/learner/Agent0/num_agent_steps_trained,info/learner/Agent0/num_grad_updates_lifetime,info/learner/Agent0/diff_num_grad_updates_vs_sampler_policy,info/learner/Agent1/learner_stats/allreduce_latency,info/learner/Agent1/learner_stats/grad_gnorm,info/learner/Agent1/learner_stats/cur_kl_coeff,info/learner/Agent1/learner_stats/cur_lr,info/learner/Agent1/learner_stats/total_loss,info/learner/Agent1/learner_stats/policy_loss,info/learner/Agent1/learner_stats/vf_loss,info/learner/Agent1/learner_stats/vf_explained_var,info/learner/Agent1/learner_stats/kl,info/learner/Agent1/learner_stats/entropy,info/learner/Agent1/learner_stats/entropy_coeff,info/learner/Agent1/num_agent_steps_trained,info/learner/Agent1/num_grad_updates_lifetime,info/learner/Agent1/diff_num_grad_updates_vs_sampler_policy,info/learner/Agent4/learner_stats/allreduce_latency,info/learner/Agent4/learner_stats/grad_gnorm,info/learner/Agent4/learner_stats/cur_kl_coeff,info/learner/Agent4/learner_stats/cur_lr,info/learner/Agent4/learner_stats/total_loss,info/learner/Agent4/learner_stats/policy_loss,info/learner/Agent4/learner_stats/vf_loss,info/learner/Agent4/learner_stats/vf_explained_var,info/learner/Agent4/learner_stats/kl,info/learner/Agent4/learner_stats/entropy,info/learner/Agent4/learner_stats/entropy_coeff,info/learner/Agent4/num_agent_steps_trained,info/learner/Agent4/num_grad_updates_lifetime,info/learner/Agent4/diff_num_grad_updates_vs_sampler_policy,info/learner/Agent3/learner_stats/allreduce_latency,info/learner/Agent3/learner_stats/grad_gnorm,info/learner/Agent3/learner_stats/cur_kl_coeff,info/learner/Agent3/learner_stats/cur_lr,info/learner/Agent3/learner_stats/total_loss,info/learner/Agent3/learner_stats/policy_loss,info/learner/Agent3/learner_stats/vf_loss,info/learner/Agent3/learner_stats/vf_explained_var,info/learner/Agent3/learner_stats/kl,info/learner/Agent3/learner_stats/entropy,info/learner/Agent3/learner_stats/entropy_coeff,info/learner/Agent3/num_agent_steps_trained,info/learner/Agent3/num_grad_updates_lifetime,info/learner/Agent3/diff_num_grad_updates_vs_sampler_policy,info/learner/Agent2/learner_stats/allreduce_latency,info/learner/Agent2/learner_stats/grad_gnorm,info/learner/Agent2/learner_stats/cur_kl_coeff,info/learner/Agent2/learner_stats/cur_lr,info/learner/Agent2/learner_stats/total_loss,info/learner/Agent2/learner_stats/policy_loss,info/learner/Agent2/learner_stats/vf_loss,info/learner/Agent2/learner_stats/vf_explained_var,info/learner/Agent2/learner_stats/kl,info/learner/Agent2/learner_stats/entropy,info/learner/Agent2/learner_stats/entropy_coeff,info/learner/Agent2/num_agent_steps_trained,info/learner/Agent2/num_grad_updates_lifetime,info/learner/Agent2/diff_num_grad_updates_vs_sampler_policy,info/num_env_steps_sampled,info/num_env_steps_trained,info/num_agent_steps_sampled,info/num_agent_steps_trained,env_runners/episode_reward_max,env_runners/episode_reward_min,env_runners/episode_reward_mean,env_runners/episode_len_mean,env_runners/episodes_timesteps_total,env_runners/policy_reward_min/Agent0,env_runners/policy_reward_min/Agent1,env_runners/policy_reward_min/Agent2,env_runners/policy_reward_min/Agent3,env_runners/policy_reward_min/Agent4,env_runners/policy_reward_max/Agent0,env_runners/policy_reward_max/Agent1,env_runners/policy_reward_max/Agent2,env_runners/policy_reward_max/Agent3,env_runners/policy_reward_max/Agent4,env_runners/policy_reward_mean/Agent0,env_runners/policy_reward_mean/Agent1,env_runners/policy_reward_mean/Agent2,env_runners/policy_reward_mean/Agent3,env_runners/policy_reward_mean/Agent4,env_runners/hist_stats/episode_reward,env_runners/hist_stats/episode_lengths,env_runners/hist_stats/policy_Agent0_reward,env_runners/hist_stats/policy_Agent1_reward,env_runners/hist_stats/policy_Agent2_reward,env_runners/hist_stats/policy_Agent3_reward,env_runners/hist_stats/policy_Agent4_reward,env_runners/sampler_perf/mean_raw_obs_processing_ms,env_runners/sampler_perf/mean_inference_ms,env_runners/sampler_perf/mean_action_processing_ms,env_runners/sampler_perf/mean_env_wait_ms,env_runners/sampler_perf/mean_env_render_ms,env_runners/num_faulty_episodes,env_runners/connector_metrics/StateBufferConnector_ms,env_runners/connector_metrics/ViewRequirementAgentConnector_ms,env_runners/num_episodes,env_runners/episode_return_max,env_runners/episode_return_min,env_runners/episode_return_mean,env_runners/episodes_this_iter,timers/training_iteration_time_ms,timers/restore_workers_time_ms,timers/training_step_time_ms,timers/sample_time_ms,timers/learn_time_ms,timers/learn_throughput,timers/synch_weights_time_ms,counters/num_env_steps_sampled,counters/num_env_steps_trained,counters/num_agent_steps_sampled,counters/num_agent_steps_trained,config/placement_strategy,config/num_gpus,config/_fake_gpus,config/num_cpus_for_main_process,config/eager_tracing,config/eager_max_retraces,config/tf_session_args/intra_op_parallelism_threads,config/tf_session_args/inter_op_parallelism_threads,config/tf_session_args/gpu_options/allow_growth,config/tf_session_args/log_device_placement,config/tf_session_args/device_count/CPU,config/tf_session_args/allow_soft_placement,config/local_tf_session_args/intra_op_parallelism_threads,config/local_tf_session_args/inter_op_parallelism_threads,config/torch_compile_learner,config/torch_compile_learner_what_to_compile,config/torch_compile_learner_dynamo_backend,config/torch_compile_learner_dynamo_mode,config/torch_compile_worker,config/torch_compile_worker_dynamo_backend,config/torch_compile_worker_dynamo_mode,config/torch_skip_nan_gradients,config/enable_rl_module_and_learner,config/enable_env_runner_and_connector_v2,config/env,config/observation_space,config/action_space,config/clip_rewards,config/normalize_actions,config/clip_actions,config/_is_atari,config/disable_env_checking,config/env_task_fn,config/render_env,config/action_mask_key,config/env_runner_cls,config/num_env_runners,config/num_envs_per_env_runner,config/num_cpus_per_env_runner,config/num_gpus_per_env_runner,config/validate_env_runners_after_construction,config/max_requests_in_flight_per_env_runner,config/sample_timeout_s,config/_env_to_module_connector,config/add_default_connectors_to_env_to_module_pipeline,config/_module_to_env_connector,config/add_default_connectors_to_module_to_env_pipeline,config/episode_lookback_horizon,config/rollout_fragment_length,config/batch_mode,config/compress_observations,config/remote_worker_envs,config/remote_env_batch_wait_ms,config/enable_tf1_exec_eagerly,config/sample_collector,config/preprocessor_pref,config/observation_filter,config/update_worker_filter_stats,config/use_worker_filter_stats,config/enable_connectors,config/sampler_perf_stats_ema_coef,config/num_learners,config/num_gpus_per_learner,config/num_cpus_per_learner,config/local_gpu_idx,config/gamma,config/lr,config/grad_clip,config/grad_clip_by,config/train_batch_size_per_learner,config/train_batch_size,config/num_epochs,config/minibatch_size,config/shuffle_batch_per_epoch,config/model/fcnet_hiddens,config/model/fcnet_activation,config/model/fcnet_weights_initializer,config/model/fcnet_weights_initializer_config,config/model/fcnet_bias_initializer,config/model/fcnet_bias_initializer_config,config/model/conv_filters,config/model/conv_activation,config/model/conv_kernel_initializer,config/model/conv_kernel_initializer_config,config/model/conv_bias_initializer,config/model/conv_bias_initializer_config,config/model/conv_transpose_kernel_initializer,config/model/conv_transpose_kernel_initializer_config,config/model/conv_transpose_bias_initializer,config/model/conv_transpose_bias_initializer_config,config/model/post_fcnet_hiddens,config/model/post_fcnet_activation,config/model/post_fcnet_weights_initializer,config/model/post_fcnet_weights_initializer_config,config/model/post_fcnet_bias_initializer,config/model/post_fcnet_bias_initializer_config,config/model/free_log_std,config/model/log_std_clip_param,config/model/no_final_linear,config/model/vf_share_layers,config/model/use_lstm,config/model/max_seq_len,config/model/lstm_cell_size,config/model/lstm_use_prev_action,config/model/lstm_use_prev_reward,config/model/lstm_weights_initializer,config/model/lstm_weights_initializer_config,config/model/lstm_bias_initializer,config/model/lstm_bias_initializer_config,config/model/_time_major,config/model/use_attention,config/model/attention_num_transformer_units,config/model/attention_dim,config/model/attention_num_heads,config/model/attention_head_dim,config/model/attention_memory_inference,config/model/attention_memory_training,config/model/attention_position_wise_mlp_dim,config/model/attention_init_gru_gate_bias,config/model/attention_use_n_prev_actions,config/model/attention_use_n_prev_rewards,config/model/framestack,config/model/dim,config/model/grayscale,config/model/zero_mean,config/model/custom_model,config/model/custom_action_dist,config/model/custom_preprocessor,config/model/encoder_latent_dim,config/model/always_check_shapes,config/model/lstm_use_prev_action_reward,config/model/_use_default_native_models,config/model/_disable_preprocessor_api,config/model/_disable_action_flattening,config/_learner_connector,config/add_default_connectors_to_learner_pipeline,config/_learner_class,config/explore,config/exploration_config/type,config/count_steps_by,config/policy_map_capacity,config/policy_mapping_fn,config/policies_to_train,config/policy_states_are_swappable,config/observation_fn,config/input_read_method,config/input_read_episodes,config/input_read_sample_batches,config/input_filesystem,config/input_compress_columns,config/input_spaces_jsonable,config/materialize_data,config/materialize_mapped_data,config/prelearner_class,config/prelearner_buffer_class,config/prelearner_module_synch_period,config/dataset_num_iters_per_learner,config/actions_in_input_normalized,config/postprocess_inputs,config/shuffle_buffer_size,config/output,config/output_compress_columns,config/output_max_file_size,config/output_max_rows_per_file,config/output_write_method,config/output_filesystem,config/output_write_episodes,config/offline_sampling,config/evaluation_interval,config/evaluation_duration,config/evaluation_duration_unit,config/evaluation_sample_timeout_s,config/evaluation_parallel_to_training,config/evaluation_force_reset_envs_before_iteration,config/evaluation_config,config/ope_split_batch_by_episode,config/evaluation_num_env_runners,config/in_evaluation,config/sync_filters_on_rollout_workers_timeout_s,config/keep_per_episode_custom_metrics,config/metrics_episode_collection_timeout_s,config/metrics_num_episodes_for_smoothing,config/min_time_s_per_iteration,config/min_train_timesteps_per_iteration,config/min_sample_timesteps_per_iteration,config/log_gradients,config/export_native_model_files,config/checkpoint_trainable_policies_only,config/logger_creator,config/logger_config,config/log_level,config/log_sys_usage,config/fake_sampler,config/seed,config/_run_training_always_in_thread,config/_evaluation_parallel_to_training_wo_thread,config/recreate_failed_env_runners,config/ignore_env_runner_failures,config/max_num_env_runner_restarts,config/delay_between_env_runner_restarts_s,config/restart_failed_sub_environments,config/num_consecutive_env_runner_failures_tolerance,config/env_runner_health_probe_timeout_s,config/env_runner_restore_timeout_s,config/_rl_module_spec,config/_AlgorithmConfig__prior_exploration_config,config/_torch_grad_scaler_class,config/_torch_lr_scheduler_classes,config/_tf_policy_handles_more_than_one_loss,config/_disable_preprocessor_api,config/_disable_action_flattening,config/_disable_initialize_loss_from_dummy_batch,config/_dont_auto_sync_env_runner_states,config/simple_optimizer,config/policy_map_cache,config/worker_cls,config/synchronize_filters,config/enable_async_evaluation,config/custom_async_evaluation_function,config/_enable_rl_module_api,config/auto_wrap_old_gym_envs,config/always_attach_evaluation_results,config/replay_sequence_length,config/_disable_execution_plan_api,config/lr_schedule,config/use_critic,config/use_gae,config/use_kl_loss,config/kl_coeff,config/kl_target,config/vf_loss_coeff,config/entropy_coeff,config/entropy_coeff_schedule,config/clip_param,config/vf_clip_param,config/sgd_minibatch_size,config/vf_share_layers,config/training/train_batch_size,config/training/minibatch_size,config/__stdout_file__,config/__stderr_file__,config/lambda,config/input,config/policies/Agent0,config/policies/Agent1,config/policies/Agent2,config/policies/Agent3,config/policies/Agent4,config/callbacks,config/create_env_on_driver,config/custom_eval_function,config/framework,perf/cpu_util_percent,perf/ram_util_percent,perf/gpu_util_percent0,perf/vram_util_percent0,logdir
30,0,0,2869250,2869250,573850,573850,11477,11477,263.24168278176643,263.24168278176643,573850,573850,2869250,11477,2869250,True,50,f6e9316a,2025-05-13_16-20-15,1747146015,43.60727667808533,2067.2929723262787,208902,v100v2gpu13,10.5.3.122,2067.2929723262787,50,0.0,0.13947220823417109,7.105427357601003e-16,5.0000000000000016e-05,9.917330535252889,-0.007728542974897816,9.925991519292195,-0.001269445816675822,0.0038124851103096564,0.9325195660193761,0.0010000000000000005,2869.25,5940.5,59.5,0.0,0.12291404232382774,1.1368683772161605e-14,5.0000000000000016e-05,9.919898915290833,-0.007684803173287946,9.928426877657573,-0.0004075556993484497,0.0028985752121419256,0.8431818947196007,0.0010000000000000005,2869.25,5940.5,59.5,0.0,0.1416902889808019,1.1641532182693484e-11,5.0000000000000016e-05,9.90898543993632,-0.016118922747167138,9.926552812258402,-0.0009087512890497844,0.004517360742026236,1.448400726914406,0.0010000000000000005,2869.25,5940.5,59.5,0.0,0.13352213048686584,7.275957614183427e-13,5.0000000000000016e-05,9.92010748386383,-0.00787710930923519,9.92898613611857,-0.001143487294514974,0.004385758519917241,1.0015522281328837,0.0010000000000000005,2869.25,5940.5,59.5,0.0,0.1370476436490814,1.4210854715202006e-15,5.0000000000000016e-05,9.916374945640564,-0.008948599243497786,9.926305047671,-0.0022166003783543903,0.004687603163673278,0.9815600901842118,0.0010000000000000005,2869.25,5940.5,59.5,573850,573850,2869250,2869250,-1785.0,-7810.0,-3801.7,499.0,49900,-1562.0,-1562.0,-1562.0,-1562.0,-1562.0,-357.0,-357.0,-357.0,-357.0,-357.0,-760.34,-760.34,-760.34,-760.34,-760.34,"[-3440.0, -3160.0, -2315.0, -2095.0, -3480.0, -2050.0, -5480.0, -3700.0, -4630.0, -3890.0, -2155.0, -2785.0, -1915.0, -2950.0, -2990.0, -1785.0, -3360.0, -3355.0, -2845.0, -6475.0, -5100.0, -2595.0, -3565.0, -5230.0, -3495.0, -2730.0, -5840.0, -2090.0, -5390.0, -5540.0, -2575.0, -3355.0, -4535.0, -2225.0, -2905.0, -5205.0, -5810.0, -3930.0, -3005.0, -5655.0, -5275.0, -7785.0, -2690.0, -4580.0, -2805.0, -3530.0, -3970.0, -2680.0, -2840.0, -2435.0, -3915.0, -2820.0, -2885.0, -2820.0, -4630.0, -4550.0, -3070.0, -6425.0, -5145.0, -2305.0, -3690.0, -3905.0, -4270.0, -3875.0, -4610.0, -3920.0, -3565.0, -5835.0, -2875.0, -4310.0, -4030.0, -3610.0, -7435.0, -2720.0, -3130.0, -2765.0, -5315.0, -2760.0, -2655.0, -3625.0, -2745.0, -4410.0, -3145.0, -3745.0, -4275.0, -4320.0, -3830.0, -2540.0, -7810.0, -2740.0, -4410.0, -6230.0, -2425.0, -2920.0, -4235.0, -2150.0, -4510.0, -6505.0, -2800.0, -2740.0]","[499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499, 499]","[-688.0, -632.0, -463.0, -419.0, -696.0, -410.0, -1096.0, -740.0, -926.0, -778.0, -431.0, -557.0, -383.0, -590.0, -598.0, -357.0, -672.0, -671.0, -569.0, -1295.0, -1020.0, -519.0, -713.0, -1046.0, -699.0, -546.0, -1168.0, -418.0, -1078.0, -1108.0, -515.0, -671.0, -907.0, -445.0, -581.0, -1041.0, -1162.0, -786.0, -601.0, -1131.0, -1055.0, -1557.0, -538.0, -916.0, -561.0, -706.0, -794.0, -536.0, -568.0, -487.0, -783.0, -564.0, -577.0, -564.0, -926.0, -910.0, -614.0, -1285.0, -1029.0, -461.0, -738.0, -781.0, -854.0, -775.0, -922.0, -784.0, -713.0, -1167.0, -575.0, -862.0, -806.0, -722.0, -1487.0, -544.0, -626.0, -553.0, -1063.0, -552.0, -531.0, -725.0, -549.0, -882.0, -629.0, -749.0, -855.0, -864.0, -766.0, -508.0, -1562.0, -548.0, -882.0, -1246.0, -485.0, -584.0, -847.0, -430.0, -902.0, -1301.0, -560.0, -548.0]","[-688.0, -632.0, -463.0, -419.0, -696.0, -410.0, -1096.0, -740.0, -926.0, -778.0, -431.0, -557.0, -383.0, -590.0, -598.0, -357.0, -672.0, -671.0, -569.0, -1295.0, -1020.0, -519.0, -713.0, -1046.0, -699.0, -546.0, -1168.0, -418.0, -1078.0, -1108.0, -515.0, -671.0, -907.0, -445.0, -581.0, -1041.0, -1162.0, -786.0, -601.0, -1131.0, -1055.0, -1557.0, -538.0, -916.0, -561.0, -706.0, -794.0, -536.0, -568.0, -487.0, -783.0, -564.0, -577.0, -564.0, -926.0, -910.0, -614.0, -1285.0, -1029.0, -461.0, -738.0, -781.0, -854.0, -775.0, -922.0, -784.0, -713.0, -1167.0, -575.0, -862.0, -806.0, -722.0, -1487.0, -544.0, -626.0, -553.0, -1063.0, -552.0, -531.0, -725.0, -549.0, -882.0, -629.0, -749.0, -855.0, -864.0, -766.0, -508.0, -1562.0, -548.0, -882.0, -1246.0, -485.0, -584.0, -847.0, -430.0, -902.0, -1301.0, -560.0, -548.0]","[-688.0, -632.0, -463.0, -419.0, -696.0, -410.0, -1096.0, -740.0, -926.0, -778.0, -431.0, -557.0, -383.0, -590.0, -598.0, -357.0, -672.0, -671.0, -569.0, -1295.0, -1020.0, -519.0, -713.0, -1046.0, -699.0, -546.0, -1168.0, -418.0, -1078.0, -1108.0, -515.0, -671.0, -907.0, -445.0, -581.0, -1041.0, -1162.0, -786.0, -601.0, -1131.0, -1055.0, -1557.0, -538.0, -916.0, -561.0, -706.0, -794.0, -536.0, -568.0, -487.0, -783.0, -564.0, -577.0, -564.0, -926.0, -910.0, -614.0, -1285.0, -1029.0, -461.0, -738.0, -781.0, -854.0, -775.0, -922.0, -784.0, -713.0, -1167.0, -575.0, -862.0, -806.0, -722.0, -1487.0, -544.0, -626.0, -553.0, -1063.0, -552.0, -531.0, -725.0, -549.0, -882.0, -629.0, -749.0, -855.0, -864.0, -766.0, -508.0, -1562.0, -548.0, -882.0, -1246.0, -485.0, -584.0, -847.0, -430.0, -902.0, -1301.0, -560.0, -548.0]","[-688.0, -632.0, -463.0, -419.0, -696.0, -410.0, -1096.0, -740.0, -926.0, -778.0, -431.0, -557.0, -383.0, -590.0, -598.0, -357.0, -672.0, -671.0, -569.0, -1295.0, -1020.0, -519.0, -713.0, -1046.0, -699.0, -546.0, -1168.0, -418.0, -1078.0, -1108.0, -515.0, -671.0, -907.0, -445.0, -581.0, -1041.0, -1162.0, -786.0, -601.0, -1131.0, -1055.0, -1557.0, -538.0, -916.0, -561.0, -706.0, -794.0, -536.0, -568.0, -487.0, -783.0, -564.0, -577.0, -564.0, -926.0, -910.0, -614.0, -1285.0, -1029.0, -461.0, -738.0, -781.0, -854.0, -775.0, -922.0, -784.0, -713.0, -1167.0, -575.0, -862.0, -806.0, -722.0, -1487.0, -544.0, -626.0, -553.0, -1063.0, -552.0, -531.0, -725.0, -549.0, -882.0, -629.0, -749.0, -855.0, -864.0, -766.0, -508.0, -1562.0, -548.0, -882.0, -1246.0, -485.0, -584.0, -847.0, -430.0, -902.0, -1301.0, -560.0, -548.0]","[-688.0, -632.0, -463.0, -419.0, -696.0, -410.0, -1096.0, -740.0, -926.0, -778.0, -431.0, -557.0, -383.0, -590.0, -598.0, -357.0, -672.0, -671.0, -569.0, -1295.0, -1020.0, -519.0, -713.0, -1046.0, -699.0, -546.0, -1168.0, -418.0, -1078.0, -1108.0, -515.0, -671.0, -907.0, -445.0, -581.0, -1041.0, -1162.0, -786.0, -601.0, -1131.0, -1055.0, -1557.0, -538.0, -916.0, -561.0, -706.0, -794.0, -536.0, -568.0, -487.0, -783.0, -564.0, -577.0, -564.0, -926.0, -910.0, -614.0, -1285.0, -1029.0, -461.0, -738.0, -781.0, -854.0, -775.0, -922.0, -784.0, -713.0, -1167.0, -575.0, -862.0, -806.0, -722.0, -1487.0, -544.0, -626.0, -553.0, -1063.0, -552.0, -531.0, -725.0, -549.0, -882.0, -629.0, -749.0, -855.0, -864.0, -766.0, -508.0, -1562.0, -548.0, -882.0, -1246.0, -485.0, -584.0, -847.0, -430.0, -902.0, -1301.0, -560.0, -548.0]",1.7907553986118254,7.133769339637735,0.4613991805735905,45.82745175026125,0.0,0,0.005087041854858398,0.10445189476013184,30,-1785.0,-7810.0,-3801.7,30,42186.951,0.032,42186.864,34255.182,7864.766,1459.293,58.662,573850,573850,2869250,2869250,PACK,1,False,1,True,20,2,2,True,False,1,True,8,8,False,forward_train,inductor,,False,onnxrt,,False,False,False,CC4,,,,True,False,,False,,False,action_mask,,30,1,1,0,True,2,,,True,,True,1,auto,complete_episodes,False,False,0,False,<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,deepmind,NoFilter,True,True,True,,0,0,1,0,0.99,5e-05,,global_norm,,11364,30,3255,True,"[256, 256]",tanh,,,,,,relu,,,,,,,,,[],relu,,,,,False,20.0,False,False,False,20,256,False,False,,,,,False,False,1,64,1,32,50,50,32,2.0,0,0,True,84,False,True,my_model,,,,False,-1,-1,False,False,,True,,True,StochasticSampling,env_steps,100,<function policy_mapper at 0x7fd63b26d4e0>,,False,,read_parquet,False,False,,"['obs', 'new_obs']",True,False,True,,,10,,False,False,0,,"['obs', 'new_obs']",67108864,,write_parquet,,True,False,,10,episodes,120.0,False,True,,True,0,False,10.0,False,60.0,100,,0,0,True,False,False,,,INFO,True,False,,False,False,False,False,1000,60.0,False,100,30,1800,,,,,False,True,False,False,False,-1,-1,-1,-1,-1,-1,-1,-1,-1,,-1,,True,True,True,0.2,0.01,1.0,0.0,,0.3,10.0,-1,-1,11364,3255,,,1.0,sampler,"(<class 'ray.rllib.algorithms.ppo.ppo_torch_policy.PPOTorchPolicy'>, Dict('action_mask': MultiDiscrete([2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2]), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4])), Discrete(82), {'entropy_coeff': 0.001})","(<class 'ray.rllib.algorithms.ppo.ppo_torch_policy.PPOTorchPolicy'>, Dict('action_mask': MultiDiscrete([2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2]), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4])), Discrete(82), {'entropy_coeff': 0.001})","(<class 'ray.rllib.algorithms.ppo.ppo_torch_policy.PPOTorchPolicy'>, Dict('action_mask': MultiDiscrete([2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2]), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4])), Discrete(82), {'entropy_coeff': 0.001})","(<class 'ray.rllib.algorithms.ppo.ppo_torch_policy.PPOTorchPolicy'>, Dict('action_mask': MultiDiscrete([2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2]), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4])), Discrete(82), {'entropy_coeff': 0.001})","(<class 'ray.rllib.algorithms.ppo.ppo_torch_policy.PPOTorchPolicy'>, Dict('action_mask': MultiDiscrete([2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]), 'observations': MultiDiscrete([3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4
 4 4 4 4])), Discrete(242), {'entropy_coeff': 0.001})",<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>,False,,torch,68.85961538461538,86.72500000000001,0.041346153846153845,0.02056884765625,f6e9316a
